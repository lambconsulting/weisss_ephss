{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Mean/Median Age (Age_Days) - Descriptive Statistics Mean/IQR\n",
    "# 2) Mean/Median Weight (WT_KGS) - Descriptive Statistics Mean/IQR\n",
    "# 3) Gender distribution (GENDER) - significant differences? - One Way Chi Squre\n",
    "# 4) Most common Breeds (BREED) - One Way Chi Squre\n",
    "# 5) Most common shunts (SHUNT) - One Way Chi Squre\n",
    "# 6) Most common portal vein origin (PORTAL_V) - One Way Chi Squre\n",
    "# 7) Most common systemic vein insertion (SYSTEM_V) - One Way Chi Squre\n",
    "# 8) Most common contributing vessels (CONTRB_V) - One Way Chi Squre\n",
    "# 9) PP score distribution (PP_Score) - One Way Chi Squre and Descriptive Statistics Mean/IQR (age, wt, and pps_n are set as continuous exploratory)\n",
    "\n",
    "# Cross tab frequencies and ANOVA\n",
    "# HOSPITAL COMPARISONS (HOSPITAL):\n",
    "# 10) Do the above values differ AMONG HOSPITALS (HOSPITAL) - Two Way Chi Squre (Comprehensive pairwise comparisons reveals)\n",
    "\n",
    "# Cross tab frequencies and ANOVA\n",
    "# COUNTRY COMPARISONS (COUNTRY):\n",
    "# 11) Do the above values differ AMONG COUNTRIES (COUNTRY)  - Two Way Chi Squre (Comprehensive pairwise comparisons reveals)\n",
    "\n",
    "# Cross tab frequencies and ANOVA\n",
    "# BREED COMPARISONS (BREED): [5 MOST COMMON BREEDS]\n",
    "# 12) What is Gender distribution (GENDER) comparison among 5 most common breeds (BREED)? - Two Way Chi Squre (Comprehensive pairwise comparisons reveals)\n",
    "# 13) What is Age distribution (Age_Days) comparison among 5 most common breeds (BREED)? - Sub ANOVA limited to breeds 61, 38, 53, 35, 47\n",
    "# 14) What are 5 most common Shunts  (SHUNT) comparison among 5 most common breeds (BREED)? - Two Way Chi Squre (Comprehensive pairwise comparisons reveals)\n",
    "# 15) What is PP Score (PP_Score) comparison among 5 most common breeds (BREED)? # Probably best looked at categorically with two-way\n",
    "# comprehensive chi-square for contingency table first time (Comprehensive pairwise comparisons reveals)\n",
    "\n",
    "\n",
    "# Regression and ANOVA\n",
    "# AGE COMPARISONS (Age_Days):\n",
    "# 16) Does AGE (Age_Days) correlate with Breed (BREED) - RQ 13 handles this with age as DV and Breed (Top 5) as IV with 5 class levels fo breed\n",
    "# 17) Does AGE (Age_Days) correlate with PP Score (PP_Score) - Standard ANOVA but similar issue with shunt type having a lot of cats, see 18\n",
    "# 18) Does AGE (Age_Days) correlate with Shunt (SHUNT) - There are 44 different shunt types (shunt type is not ordinal) and age is continuous. \n",
    "#  An ANOVA would \n",
    "# not yield meaningful results iwth the 44 goups giving a mean age for pairise comparisons.  We could contribute collapsing shunt and \n",
    "# bucketing age.\n",
    "\n",
    "\n",
    "# SHUNT COMPARISONS (SHUNT) ANOVA:\n",
    "# 19) Does Shunt (SHUNT) correlate with Weight (WT_KGS)    [Above and below mean/median?] - see 20\n",
    "# 20) Does Shunt (SHUNT) correlate with PP Score (PP_Score) There are 44 different shunt types (non-ordinal) and 11 (with one missing) pp Score.  \n",
    "# This could be a \n",
    "# either a huge cross tab or a difficlt it interpret ANOVA.   Need to define DV/IV an consider collpasing (top 5?) Shunt Type. See 17 and 18\n",
    "\n",
    "# Breed ANOVA and Cross Tab\n",
    "# These (21, 22, 23) are all caegorical with multiple and in most cases non-ordinal categorization making inelgible for regression and \n",
    "# multiple categories difficult to interpret with thin data classes for ANOVA.  Suggest first round (Comprehensive pairwise comparisons reveals)\n",
    "# 21) Does PV origin (PORTAL_V) correlate with breed (BREED) # Cross Tab Frequency - Two Way Chi Square\n",
    "# 22) Does systemic v insertion (SYSTEM_V) correlate with breed (BREED) # Cross Tab Frequency - Two Way Chi Square\n",
    "# 23) Do contributing vessels (CONTRB_V) correlate with breed (BREED) # Cross Tab Frequency - Two Way Chi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\outdated\\utils.py:18: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.2, the latest is 0.5.3.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  **kwargs\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import os, saspy, itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "now = datetime.now()\n",
    "\n",
    "#Stats Import\n",
    "from scipy.stats import wilcoxon \n",
    "from scipy.stats import shapiro \n",
    "from scipy.stats import kstest\n",
    "# from numpy.random import seed\n",
    "# from numpy.random import math\n",
    "\n",
    "import pingouin as pg\n",
    "from pingouin import pairwise_ttests, pairwise_tukey, pairwise_gameshowell\n",
    "import statsmodels.stats.multicomp as mc\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import scipy.stats as stats\n",
    "# from scipy import stats # Might need as is for ttest proportiality z or tdist\n",
    "import statsmodels.api as sm\n",
    "from bioinfokit.analys import stat\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "from operator import concat\n",
    "\n",
    "# sys.path.append(os.path.join(sys.path[0],'C:\\Junk\\lamb_consulting'))\n",
    "sys.path.append(os.path.join(sys.path[0],'U:\\lamb_consulting'))\n",
    "\n",
    "\n",
    "# import cox, means, regressions\n",
    "\n",
    "from lamb_consulting import *\n",
    "\n",
    "from lamb_consulting.means import means_desc, means_desc_class, aov\n",
    "from lamb_consulting.freqs import freqs, chi_contingency, chi_oneway\n",
    "from lamb_consulting.export import export_to_Excel\n",
    "# from lamb_consulting.regressions import ols, regression_plot\n",
    "from lamb_consulting.cox import cox\n",
    "from lamb_consulting.kaplan_meier import km,km_plots\n",
    "\n",
    "\n",
    "def freqme(ds,var1):\n",
    "    f=ds[var1].value_counts(dropna=False)\n",
    "    p=ds[var1].value_counts(normalize=True)\n",
    "    df=pd.concat({f,p}, axis=1, keys=['frequency', 'percent'])\n",
    "    df[\"cumfrequency\"] = df[\"frequency\"].cumsum()\n",
    "    df[\"cumpercent\"] = df[\"percent\"].cumsum()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# For figure image to base 64 writing\n",
    "\n",
    "def fig_to_base64(fig):\n",
    "    img = io.BytesIO()\n",
    "    fig.savefig(img, format='png',\n",
    "                bbox_inches='tight')\n",
    "    img.seek(0)\n",
    "\n",
    "    return base64.b64encode(img.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:\\Fox\\Weisse\\EHPSS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Ken\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1082, 33)\n",
      "(520, 33)\n",
      "(123, 33)\n",
      "(417, 33)\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "# tpath = \"C:/Junk/Weisse/EHPSS/\"\n",
    "tpath = \"U:/Fox/Weisse/EHPSS/\"\n",
    "fn=\"EHPSS20221207\"\n",
    "\n",
    "extension='.xlsx'\n",
    "file = tpath+fn+extension\n",
    "\n",
    "df_in = pd.read_excel((file), sheet_name='Data')\n",
    "\n",
    "# Filter out age lt 90d \n",
    "# df = df_in.loc[(df_in['age_d']>=90)]\n",
    "# Treating lt 90 as a class\n",
    "df = df_in.copy()\n",
    "df['contrb_v'].fillna(method='ffill')\n",
    "df['wt_kg'].fillna(method='ffill')\n",
    "\n",
    "df_90_730 = df.loc[(df['age_d'] >= 90) & (df['age_d'] <= 730) ]\n",
    "df_730_1095 = df.loc[(df['age_d'] > 730) & (df['age_d'] <= 1095)]\n",
    "df_1095 = df.loc[(df['age_d' ] > 1095)]\n",
    "\n",
    "# 50%        733.000000\n",
    "def age_lt_90(row):\n",
    "    if row[\"age_d\"] <= 90:\n",
    "        return 1\n",
    "    elif row[\"age_d\"] > 90:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def age_lt_730(row):\n",
    "    if row[\"age_d\"] <= 730:\n",
    "        return 1\n",
    "    elif row[\"age_d\"] > 730:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Note changed 1 to second condition for greater than given variable name change\n",
    "def age_gt_1095(row):\n",
    "    if row[\"age_d\"] <= 1095:\n",
    "        return 0\n",
    "    elif row[\"age_d\"] > 1095:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def age_90_730(row):\n",
    "    if row[\"age_d\"] <= 90:\n",
    "        return 1\n",
    "    elif row[\"age_d\"] <= 730:\n",
    "        return 2\n",
    "    elif row[\"age_d\"] > 730:\n",
    "        return 3\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def age_730_1095(row):\n",
    "    if row[\"age_d\"] <= 730:\n",
    "        return 1\n",
    "    elif row[\"age_d\"] <= 1095:\n",
    "        return 2\n",
    "    elif row[\"age_d\"] > 1095:\n",
    "        return 3\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def male_n(row):\n",
    "    if row[\"gender\"] == 1 or row[\"gender\"] == 2:\n",
    "        return 1\n",
    "    elif row[\"gender\"] == 3 or row[\"gender\"] == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def male_c(row):\n",
    "    if row[\"gender_1\"] == 'MC' or row[\"gender_1\"] == 'MI' or row[\"gender_1\"] == 'NM' or row[\"gender_1\"] == 'MN' or row[\"gender_1\"] == 'ME' or row[\"gender_1\"] == 'M':\n",
    "        return \"M\"\n",
    "    elif row[\"gender_1\"] == 'FS' or row[\"gender_1\"] == 'FI' or row[\"gender_1\"] == 'FE': \n",
    "        return \"F\"\n",
    "    else:\n",
    "        return \"error\"\n",
    "\n",
    "# Will revisit this to loop datafrmes into apply statements, giving error as being passed in as a string\n",
    "my_dfs = ['df_90_730', 'df_730_1095', 'df_1095']\n",
    "\n",
    "df['male_n'] = df.apply(male_n,axis=1)\n",
    "df['male_c'] = df.apply(male_c,axis=1)\n",
    "df['age_lt_90'] = df.apply(age_lt_90,axis=1)\n",
    "df['age_lt_730'] = df.apply(age_lt_730,axis=1)\n",
    "df['age_gt_1095'] = df.apply(age_gt_1095,axis=1)\n",
    "df['age_90_730'] = df.apply(age_90_730,axis=1)\n",
    "df['age_730_1095'] = df.apply(age_730_1095,axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "df_90_730['male_n'] = df_90_730.apply(male_n,axis=1)\n",
    "df_90_730['male_c'] = df_90_730.apply(male_c,axis=1)\n",
    "df_90_730['age_lt_90'] = df_90_730.apply(age_lt_90,axis=1)\n",
    "df_90_730['age_lt_730'] = df_90_730.apply(age_lt_730,axis=1)\n",
    "df_90_730['age_gt_1095'] = df_90_730.apply(age_gt_1095,axis=1)\n",
    "df_90_730['age_90_730'] = df_90_730.apply(age_90_730,axis=1)\n",
    "df_90_730['age_730_1095'] = df_90_730.apply(age_730_1095,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_730_1095['male_n'] = df_730_1095.apply(male_n,axis=1)\n",
    "df_730_1095['male_c'] = df_730_1095.apply(male_c,axis=1)\n",
    "df_730_1095['age_lt_90'] = df_730_1095.apply(age_lt_90,axis=1)\n",
    "df_730_1095['age_lt_730'] = df_730_1095.apply(age_lt_730,axis=1)\n",
    "df_730_1095['age_gt_1095'] = df_730_1095.apply(age_gt_1095,axis=1)\n",
    "df_730_1095['age_90_730'] = df_730_1095.apply(age_90_730,axis=1)\n",
    "df_730_1095['age_730_1095'] = df_730_1095.apply(age_730_1095,axis=1)\n",
    "\n",
    "df_1095['male_n'] = df_1095.apply(male_n,axis=1)\n",
    "df_1095['male_c'] = df_1095.apply(male_c,axis=1)\n",
    "df_1095['age_lt_90'] = df_1095.apply(age_lt_90,axis=1)\n",
    "df_1095['age_lt_730'] = df_1095.apply(age_lt_730,axis=1)\n",
    "df_1095['age_gt_1095'] = df_1095.apply(age_gt_1095,axis=1)\n",
    "df_1095['age_90_730'] = df_1095.apply(age_90_730,axis=1)\n",
    "df_1095['age_730_1095'] = df_1095.apply(age_730_1095,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "cont = ['age_d', 'wt_kg', 'pps_n']\n",
    "cont_noage = ['wt_kg', 'pps_n']\n",
    "cat = ['male_n', 'breed_n', 'shunt_ty', 'portal_v', 'system_v', 'contrb_v', 'hospital', 'country', 'pps_n']\n",
    "# cat_nopps = ['male_n', 'breed_n', 'shunt_ty', 'portal_v', 'system_v', 'contrb_v', 'hospital', 'country']\n",
    "cat_nopps = ['male_n', 'breed_n', 'shunt_ty', 'portal_v', 'system_v', 'contrb_v', 'hospital', 'country','age_lt_90','age_lt_730','age_gt_1095','age_90_730','age_730_1095']\n",
    "\n",
    "# cont_describe = \"age_d wt_kg pps_n\"\n",
    "# cat_describe = \"male_c breed_nc shunt_ty portal_v system_v contrb_v hospital country pps_n\"\n",
    "\n",
    "\n",
    "# print(df[\"gender_1\"].value_counts())\n",
    "# print(df[\"gender\"].value_counts())\n",
    "\n",
    "# print(df[\"male_c\"].value_counts())\n",
    "# print(df[\"male_n\"].value_counts())\n",
    "\n",
    "print(df.shape)\n",
    "print(df_90_730.shape)\n",
    "print(df_730_1095.shape)\n",
    "print(df_1095.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Way Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 8)\n",
      "  df_used         table  var_level  frequency   percent  cumfrequency  \\\n",
      "0      df  Table male_n        0.0        575  0.531423           575   \n",
      "1      df  Table male_n        1.0        507  0.468577          1082   \n",
      "\n",
      "   cumpercent  p_value  \n",
      "0    0.531423  0.03871  \n",
      "1    1.000000  0.03871  \n",
      "(139, 8)\n",
      "     df_used         table  var_level  frequency   percent  cumfrequency  \\\n",
      "0  df_90_730  Table male_n        0.0        265  0.509615           265   \n",
      "1  df_90_730  Table male_n        1.0        255  0.490385           520   \n",
      "\n",
      "   cumpercent   p_value  \n",
      "0    0.509615  0.661003  \n",
      "1    1.000000  0.661003  \n",
      "(91, 8)\n",
      "       df_used         table  var_level  frequency   percent  cumfrequency  \\\n",
      "0  df_730_1095  Table male_n        0.0         80  0.650407            80   \n",
      "1  df_730_1095  Table male_n        1.0         43  0.349593           123   \n",
      "\n",
      "   cumpercent   p_value  \n",
      "0    0.650407  0.000849  \n",
      "1    1.000000  0.000849  \n",
      "(120, 8)\n",
      "   df_used         table  var_level  frequency   percent  cumfrequency  \\\n",
      "0  df_1095  Table male_n        0.0        216  0.517986           216   \n",
      "1  df_1095  Table male_n        1.0        201  0.482014           417   \n",
      "\n",
      "   cumpercent   p_value  \n",
      "0    0.517986  0.462612  \n",
      "1    1.000000  0.462612  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_list = cat.copy()\n",
    "my_list.remove(\"breed_n\")\n",
    "my_list += ['breed_n']\n",
    "\n",
    "my_list += ['age_lt_90','age_lt_730','age_gt_1095','age_90_730','age_730_1095']\n",
    "\n",
    "\n",
    "\n",
    "my_cat_df_list = ['cat_df', 'cat_df_90_730', 'cat_df_730_1095', 'cat_df_1095']\n",
    "cat_df = pd.DataFrame()\n",
    "cat_df_90_730 = pd.DataFrame()\n",
    "cat_df_730_1095 = pd.DataFrame()\n",
    "cat_df_1095 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in my_list:\n",
    "    cat_df = cat_df.append(chi_oneway(df,i,\"df\"))\n",
    "\n",
    "for i in my_list:\n",
    "    cat_df_90_730 = cat_df_90_730.append(chi_oneway(df_90_730,i,\"df_90_730\"))\n",
    "\n",
    "for i in my_list:\n",
    "    cat_df_730_1095 = cat_df_730_1095.append(chi_oneway(df_730_1095,i,\"df_730_1095\"))\n",
    "\n",
    "for i in my_list:\n",
    "    cat_df_1095 = cat_df_1095.append(chi_oneway(df_1095,i,\"df_1095\"))\n",
    "\n",
    "print(cat_df.shape)\n",
    "print(cat_df.head(2))\n",
    "print(cat_df_90_730.shape)\n",
    "print(cat_df_90_730.head(2))\n",
    "print(cat_df_730_1095.shape)\n",
    "print(cat_df_730_1095.head(2))\n",
    "print(cat_df_1095.shape)\n",
    "print(cat_df_1095.head(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Top 5 Breed dataset based of of the one-way Chi-Squre Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.0, 61.0, 53.0, 38.0, 35.0]\n",
      "(1082, 33)\n",
      "(648, 33)\n"
     ]
    }
   ],
   "source": [
    "# breed_t5_list = [62, 61, 53, 38, 35]\n",
    "breed_t5_list = df['breed_n'].value_counts().keys()[0:5].sort_values(ascending=False).tolist()\n",
    "breed_mask = (df['breed_n'].isin(breed_t5_list)) \n",
    "breed_t5 = df.loc[breed_mask]\n",
    "print(breed_t5_list)\n",
    "print(df.shape)\n",
    "print(breed_t5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Way Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "cross_cat\n",
      "  df_used r_q_n  analysis research_q                            Table  \\\n",
      "5      df     1  chi_cont          1  Table age_90_730 * age_730_1095   \n",
      "6      df     1  chi_cont          1  Table age_90_730 * age_730_1095   \n",
      "7      df     1  chi_cont          1  Table age_90_730 * age_730_1095   \n",
      "8      df     1  chi_cont          1  Table age_90_730 * age_730_1095   \n",
      "9      df     1  chi_cont          1  Table age_90_730 * age_730_1095   \n",
      "\n",
      "   Frequency     Percent Missing iv1 iv2  \n",
      "5          0    0.000000       .   2   3  \n",
      "6          0    0.000000       .   3   1  \n",
      "7        123   11.367837       .   3   2  \n",
      "8        417   38.539741       .   3   3  \n",
      "9       1082  100.000000       0   .   .  \n",
      "cross_cat_df_90_730\n",
      "     df_used r_q_n  analysis research_q                             Table  \\\n",
      "1  df_90_730     1  chi_cont          1    Table age_gt_1095 * age_90_730   \n",
      "0  df_90_730     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "1  df_90_730     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "0  df_90_730     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "1  df_90_730     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "\n",
      "   Frequency  Percent Missing iv1 iv2  \n",
      "1        520    100.0       0   .   .  \n",
      "0        520    100.0       .   0   1  \n",
      "1        520    100.0       0   .   .  \n",
      "0        520    100.0       .   2   1  \n",
      "1        520    100.0       0   .   .  \n",
      "cross_cat_df_730_1095\n",
      "       df_used r_q_n  analysis research_q                             Table  \\\n",
      "1  df_730_1095     1  chi_cont          1    Table age_gt_1095 * age_90_730   \n",
      "0  df_730_1095     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "1  df_730_1095     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "0  df_730_1095     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "1  df_730_1095     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "\n",
      "   Frequency  Percent Missing iv1 iv2  \n",
      "1        123    100.0       0   .   .  \n",
      "0        123    100.0       .   0   2  \n",
      "1        123    100.0       0   .   .  \n",
      "0        123    100.0       .   3   2  \n",
      "1        123    100.0       0   .   .  \n",
      "cross_cat_df_1095\n",
      "   df_used r_q_n  analysis research_q                             Table  \\\n",
      "1  df_1095     1  chi_cont          1    Table age_gt_1095 * age_90_730   \n",
      "0  df_1095     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "1  df_1095     1  chi_cont          1  Table age_gt_1095 * age_730_1095   \n",
      "0  df_1095     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "1  df_1095     1  chi_cont          1   Table age_90_730 * age_730_1095   \n",
      "\n",
      "   Frequency  Percent Missing iv1 iv2  \n",
      "1        417    100.0       0   .   .  \n",
      "0        417    100.0       .   1   3  \n",
      "1        417    100.0       0   .   .  \n",
      "0        417    100.0       .   3   3  \n",
      "1        417    100.0       0   .   .  \n"
     ]
    }
   ],
   "source": [
    "my_list = cat.copy()\n",
    "# my_list += ['age_lt_730']\n",
    "my_list += ['age_lt_90','age_lt_730','age_gt_1095','age_90_730','age_730_1095']\n",
    "\n",
    "iter_list = list(itertools.combinations(my_list, 2))\n",
    "iter_list\n",
    "\n",
    "# my_ct_cat_df_names = ['cross_cat', 'cross_cat_90_730', 'cross_cat_730_1095', 'cross_cat_1095']\n",
    "# my_ct_cat_df_list = [pd.DataFrame() for i in my_ct_cat_df_names]\n",
    "# my_ct_cat_df = dict(zip(my_ct_cat_df_names, my_ct_cat_df_list))\n",
    "\n",
    "# cross_cat_90_730 = pd.DataFrame()\n",
    "# cross_cat_730_1095 = pd.DataFrame()\n",
    "# cross_cat_1095 = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "print(len(iter_list))\n",
    "cross_cat = pd.DataFrame()\n",
    "cross_cat_df_90_730 = pd.DataFrame()\n",
    "cross_cat_df_730_1095 = pd.DataFrame()\n",
    "cross_cat_df_1095 = pd.DataFrame()\n",
    "\n",
    "ct = 1\n",
    "\n",
    "if ct ==1:\n",
    "    f = open('df_ct.html','w')\n",
    "\n",
    "my_len = (len(iter_list))\n",
    "for i in range(0, len(iter_list)):\n",
    "    # print(i)\n",
    "    res = [iter_list[i]]\n",
    "    a = res[0] \n",
    "    v1,v2 = a\n",
    "    cross_tmp, my_df = chi_contingency(df,v1,v2,\"1\",\"chi_cont\",\"df\")\n",
    "    cross_cat = cross_cat.append(cross_tmp)\n",
    "    if ct ==1:\n",
    "        my_html = my_df.to_html()\n",
    "        f.write(\"The Cross Tabular variables are {} and {}.\".format(v1, v2))\n",
    "        f.write(my_html)\n",
    "\n",
    "if ct ==1:\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# df_90_730\n",
    "for i in range(0, len(iter_list)):\n",
    "    # print(i)\n",
    "    res = [iter_list[i]]\n",
    "    a = res[0] \n",
    "    v1,v2 = a\n",
    "    cross_tmp, my_df = chi_contingency(df_90_730,v1,v2,\"1\",\"chi_cont\",\"df_90_730\")\n",
    "    cross_cat_df_90_730 = cross_cat_df_90_730.append(cross_tmp)\n",
    "\n",
    "\n",
    "# df_730_1095\n",
    "for i in range(0, len(iter_list)):\n",
    "    # print(i)\n",
    "    res = [iter_list[i]]\n",
    "    a = res[0] \n",
    "    v1,v2 = a\n",
    "    cross_tmp, my_df = chi_contingency(df_730_1095,v1,v2,\"1\",\"chi_cont\",\"df_730_1095\")\n",
    "    cross_cat_df_730_1095 = cross_cat_df_730_1095.append(cross_tmp)\n",
    "\n",
    "\n",
    "# df_1095\n",
    "for i in range(0, len(iter_list)):\n",
    "    # print(i)\n",
    "    res = [iter_list[i]]\n",
    "    a = res[0] \n",
    "    v1,v2 = a\n",
    "    cross_tmp, my_df = chi_contingency(df_1095,v1,v2,\"1\",\"chi_cont\",\"df_1095\")\n",
    "    cross_cat_df_1095 = cross_cat_df_1095.append(cross_tmp)\n",
    "\n",
    "print(\"cross_cat\")\n",
    "print(cross_cat.tail(5))\n",
    "print(\"cross_cat_df_90_730\")\n",
    "print(cross_cat_df_90_730.tail(5))\n",
    "print(\"cross_cat_df_730_1095\")\n",
    "print(cross_cat_df_730_1095.tail(5))\n",
    "print(\"cross_cat_df_1095\")\n",
    "print(cross_cat_df_1095.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Means first Overall then by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_desc_df\n",
      "  df_used cont_var  nmiss       n         mean  median          std   min  \\\n",
      "0      df    age_d      0  1082.0  1427.213678  722.50  3668.610398  0.00   \n",
      "0      df    wt_kg     15  1067.0     4.828744    4.44     2.627325  0.83   \n",
      "0      df    pps_n      4  1078.0     3.336735    4.00     1.630935  1.00   \n",
      "\n",
      "       max     p25      p75  \n",
      "0  41319.0  298.00  1682.50  \n",
      "0     12.0    2.61     6.35  \n",
      "0      5.0    2.00     5.00  \n",
      "m_desc_df_90_730\n",
      "     df_used cont_var  nmiss      n        mean  median         std    min  \\\n",
      "0  df_90_730    age_d      0  520.0  341.446154  306.50  168.739875  93.00   \n",
      "0  df_90_730    wt_kg      4  516.0    4.220756    3.65    2.505646   0.83   \n",
      "0  df_90_730    pps_n      3  517.0    2.733075    2.00    1.697675   1.00   \n",
      "\n",
      "     max     p25     p75  \n",
      "0  728.0  200.00  453.25  \n",
      "0   12.0    2.15    5.60  \n",
      "0    5.0    1.00    5.00  \n",
      "m_desc_df_730_1095\n",
      "       df_used cont_var  nmiss      n        mean  median         std     min  \\\n",
      "0  df_730_1095    age_d      0  123.0  882.715447   881.0  117.111935  731.00   \n",
      "0  df_730_1095    wt_kg      4  119.0    4.851176     4.7    2.495892    0.86   \n",
      "0  df_730_1095    pps_n      0  123.0    3.487805     4.0    1.511518    1.00   \n",
      "\n",
      "      max      p25     p75  \n",
      "0  1093.0  770.000  972.50  \n",
      "0    11.5    2.765    6.35  \n",
      "0     5.0    2.000    5.00  \n",
      "m_desc_df_1095\n",
      "   df_used cont_var  nmiss      n         mean   median          std     min  \\\n",
      "0  df_1095    age_d      0  417.0  3015.305036  2020.00  5544.501396  1096.0   \n",
      "0  df_1095    wt_kg      6  411.0     5.615036     5.32     2.601492     1.2   \n",
      "0  df_1095    pps_n      1  416.0     4.091346     5.00     1.196913     1.0   \n",
      "\n",
      "       max      p25      p75  \n",
      "0  41319.0  1476.00  2770.00  \n",
      "0     12.0     3.52     7.35  \n",
      "0      5.0     3.00     5.00  \n",
      "m_class_df\n",
      "   df_used cat_var  level    var  nmiss      n         mean  median  \\\n",
      "0       df  male_n    1.0  age_d    0.0  507.0  1355.731755   693.0   \n",
      "1       df  male_n    0.0  age_d    0.0  575.0  1490.242087   733.0   \n",
      "0       df  male_n    1.0  wt_kg    4.0  503.0     5.216183     4.8   \n",
      "1       df  male_n    0.0  wt_kg   11.0  564.0     4.483209     4.0   \n",
      "0       df  male_n    1.0  pps_n    2.0  505.0     3.443564     4.0   \n",
      "..     ...     ...    ...    ...    ...    ...          ...     ...   \n",
      "1       df   pps_n    5.0  pps_n    0.0  413.0     5.000000     5.0   \n",
      "2       df   pps_n    3.0  pps_n    0.0  137.0     3.000000     3.0   \n",
      "3       df   pps_n    4.0  pps_n    0.0  167.0     4.000000     4.0   \n",
      "4       df   pps_n    2.0  pps_n    0.0   92.0     2.000000     2.0   \n",
      "5       df   pps_n    NaN  pps_n    0.0    0.0          NaN     NaN   \n",
      "\n",
      "            std   min      max       p25       p75  \n",
      "0   3125.421391  0.00  41319.0  306.0000  1806.000  \n",
      "1   4089.853875  0.00  41317.0  295.0000  1623.000  \n",
      "0      2.707534  0.83     12.0    2.9400     7.125  \n",
      "1      2.505961  0.86     11.9    2.4025     6.000  \n",
      "0      1.602727  1.00      5.0    2.0000     5.000  \n",
      "..          ...   ...      ...       ...       ...  \n",
      "1      0.000000  5.00      5.0    5.0000     5.000  \n",
      "2      0.000000  3.00      3.0    3.0000     3.000  \n",
      "3      0.000000  4.00      4.0    4.0000     4.000  \n",
      "4      0.000000  2.00      2.0    2.0000     2.000  \n",
      "5           NaN   NaN      NaN       NaN       NaN  \n",
      "\n",
      "[492 rows x 13 columns]\n",
      "m_class_df_90_730\n",
      "      df_used cat_var  level    var  nmiss      n        mean  median  \\\n",
      "0   df_90_730  male_n    1.0  age_d    0.0  255.0  350.411765   318.0   \n",
      "1   df_90_730  male_n    0.0  age_d    0.0  265.0  332.818868   298.0   \n",
      "0   df_90_730  male_n    1.0  wt_kg    2.0  253.0    4.533123     4.1   \n",
      "1   df_90_730  male_n    0.0  wt_kg    2.0  263.0    3.920266     3.3   \n",
      "0   df_90_730  male_n    1.0  pps_n    2.0  253.0    2.845850     3.0   \n",
      "..        ...     ...    ...    ...    ...    ...         ...     ...   \n",
      "1   df_90_730   pps_n    5.0  pps_n    0.0  137.0    5.000000     5.0   \n",
      "2   df_90_730   pps_n    3.0  pps_n    0.0   52.0    3.000000     3.0   \n",
      "3   df_90_730   pps_n    2.0  pps_n    0.0   46.0    2.000000     2.0   \n",
      "4   df_90_730   pps_n    4.0  pps_n    0.0   66.0    4.000000     4.0   \n",
      "5   df_90_730   pps_n    NaN  pps_n    0.0    0.0         NaN     NaN   \n",
      "\n",
      "           std    min    max     p25     p75  \n",
      "0   173.765973  95.00  728.0  205.00  474.50  \n",
      "1   163.622687  93.00  717.0  198.00  440.00  \n",
      "0     2.546382   0.83   12.0    2.35    5.80  \n",
      "1     2.432987   0.90   11.9    2.00    5.35  \n",
      "0     1.653505   1.00    5.0    1.00    5.00  \n",
      "..         ...    ...    ...     ...     ...  \n",
      "1     0.000000   5.00    5.0    5.00    5.00  \n",
      "2     0.000000   3.00    3.0    3.00    3.00  \n",
      "3     0.000000   2.00    2.0    2.00    2.00  \n",
      "4     0.000000   4.00    4.0    4.00    4.00  \n",
      "5          NaN    NaN    NaN     NaN     NaN  \n",
      "\n",
      "[402 rows x 13 columns]\n",
      "m_class_df_730_1095\n",
      "        df_used cat_var  level    var  nmiss     n        mean  median  \\\n",
      "0   df_730_1095  male_n    0.0  age_d    0.0  80.0  867.250000  851.00   \n",
      "1   df_730_1095  male_n    1.0  age_d    0.0  43.0  911.488372  937.00   \n",
      "0   df_730_1095  male_n    0.0  wt_kg    4.0  76.0    4.235132    3.95   \n",
      "1   df_730_1095  male_n    1.0  wt_kg    0.0  43.0    5.940000    5.81   \n",
      "0   df_730_1095  male_n    0.0  pps_n    0.0  80.0    3.437500    4.00   \n",
      "..          ...     ...    ...    ...    ...   ...         ...     ...   \n",
      "0   df_730_1095   pps_n    3.0  pps_n    0.0  19.0    3.000000    3.00   \n",
      "1   df_730_1095   pps_n    4.0  pps_n    0.0  22.0    4.000000    4.00   \n",
      "2   df_730_1095   pps_n    5.0  pps_n    0.0  47.0    5.000000    5.00   \n",
      "3   df_730_1095   pps_n    1.0  pps_n    0.0  21.0    1.000000    1.00   \n",
      "4   df_730_1095   pps_n    2.0  pps_n    0.0  14.0    2.000000    2.00   \n",
      "\n",
      "           std     min      max       p25       p75  \n",
      "0   117.432609  731.00  1093.00  756.7500   938.500  \n",
      "1   112.236861  731.00  1087.00  814.0000  1008.500  \n",
      "0     2.201267    0.86    11.50    2.4025     5.725  \n",
      "1     2.636012    1.10    11.35    4.1750     8.175  \n",
      "0     1.499736    1.00     5.00    2.0000     5.000  \n",
      "..         ...     ...      ...       ...       ...  \n",
      "0     0.000000    3.00     3.00    3.0000     3.000  \n",
      "1     0.000000    4.00     4.00    4.0000     4.000  \n",
      "2     0.000000    5.00     5.00    5.0000     5.000  \n",
      "3     0.000000    1.00     1.00    1.0000     1.000  \n",
      "4     0.000000    2.00     2.00    2.0000     2.000  \n",
      "\n",
      "[258 rows x 13 columns]\n",
      "m_class_df_1095\n",
      "    df_used cat_var  level    var  nmiss      n         mean   median  \\\n",
      "0   df_1095  male_n    0.0  age_d    0.0  216.0  3235.852778  1972.00   \n",
      "1   df_1095  male_n    1.0  age_d    0.0  201.0  2778.298507  2116.00   \n",
      "0   df_1095  male_n    0.0  wt_kg    4.0  212.0     5.261792     4.92   \n",
      "1   df_1095  male_n    1.0  wt_kg    2.0  199.0     5.991357     5.80   \n",
      "0   df_1095  male_n    0.0  pps_n    1.0  215.0     3.967442     4.00   \n",
      "..      ...     ...    ...    ...    ...    ...          ...      ...   \n",
      "1   df_1095   pps_n    4.0  pps_n    0.0   78.0     4.000000     4.00   \n",
      "2   df_1095   pps_n    3.0  pps_n    0.0   64.0     3.000000     3.00   \n",
      "3   df_1095   pps_n    2.0  pps_n    0.0   28.0     2.000000     2.00   \n",
      "4   df_1095   pps_n    1.0  pps_n    0.0   22.0     1.000000     1.00   \n",
      "5   df_1095   pps_n    NaN  pps_n    0.0    0.0          NaN      NaN   \n",
      "\n",
      "            std     min      max        p25      p75  \n",
      "0   6294.636243  1096.0  41317.0  1495.2500  2762.00  \n",
      "1   4608.600299  1096.0  41319.0  1462.0000  2770.00  \n",
      "0      2.450901     1.2     11.9     3.3375     6.90  \n",
      "1      2.708653     1.5     12.0     3.7750     8.05  \n",
      "0      1.243247     1.0      5.0     3.0000     5.00  \n",
      "..          ...     ...      ...        ...      ...  \n",
      "1      0.000000     4.0      4.0     4.0000     4.00  \n",
      "2      0.000000     3.0      3.0     3.0000     3.00  \n",
      "3      0.000000     2.0      2.0     2.0000     2.00  \n",
      "4      0.000000     1.0      1.0     1.0000     1.00  \n",
      "5           NaN     NaN      NaN        NaN      NaN  \n",
      "\n",
      "[345 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "m_desc_df = pd.DataFrame()\n",
    "m_desc_df_90_730 = pd.DataFrame()\n",
    "m_desc_df_730_1095 = pd.DataFrame()\n",
    "m_desc_df_1095 = pd.DataFrame()\n",
    "\n",
    "m_class_df = pd.DataFrame()\n",
    "m_class_df_90_730 = pd.DataFrame()\n",
    "m_class_df_730_1095 = pd.DataFrame()\n",
    "m_class_df_1095 = pd.DataFrame()\n",
    "\n",
    "for i in cont:\n",
    "        desc_tmp = means_desc(df, i,\"df\")\n",
    "        m_desc_df = m_desc_df.append(desc_tmp)\n",
    "# df_90_730\n",
    "        desc_tmp = means_desc(df_90_730, i,\"df_90_730\")\n",
    "        m_desc_df_90_730 = m_desc_df_90_730.append(desc_tmp)\n",
    "# df_730_1095\n",
    "        desc_tmp = means_desc(df_730_1095, i,\"df_730_1095\")\n",
    "        m_desc_df_730_1095 = m_desc_df_730_1095.append(desc_tmp)\n",
    "# df_1095\n",
    "        desc_tmp = means_desc(df_1095, i,\"df_1095\")\n",
    "        m_desc_df_1095 = m_desc_df_1095.append(desc_tmp)\n",
    "print(\"m_desc_df\")\n",
    "print(m_desc_df)\n",
    "print(\"m_desc_df_90_730\")\n",
    "print(m_desc_df_90_730)\n",
    "print(\"m_desc_df_730_1095\")\n",
    "print(m_desc_df_730_1095)\n",
    "print(\"m_desc_df_1095\")\n",
    "print(m_desc_df_1095)\n",
    "\n",
    "for i in cat:\n",
    "    for j in cont:\n",
    "        desc_class_tmp = means_desc_class(df,j, i,\"df\")\n",
    "        m_class_df = m_class_df.append(desc_class_tmp)\n",
    "# df_90_730\n",
    "        desc_class_tmp = means_desc_class(df_90_730,j, i,\"df_90_730\")\n",
    "        m_class_df_90_730 = m_class_df_90_730.append(desc_class_tmp)\n",
    "# df_730_1095\n",
    "        desc_class_tmp = means_desc_class(df_730_1095,j, i,\"df_730_1095\")\n",
    "        m_class_df_730_1095 = m_class_df_730_1095.append(desc_class_tmp)\n",
    "# df_1095\n",
    "        desc_class_tmp = means_desc_class(df_1095,j, i,\"df_1095\")\n",
    "        m_class_df_1095 = m_class_df_1095.append(desc_class_tmp)\n",
    "\n",
    "print(\"m_class_df\")       \n",
    "print(m_class_df)\n",
    "print(\"m_class_df_90_730\")\n",
    "print(m_class_df_90_730)\n",
    "print(\"m_class_df_730_1095\")\n",
    "print(m_class_df_730_1095)\n",
    "print(\"m_class_df_1095\")\n",
    "print(m_class_df_1095)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffs with Tukeys Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up affectionate naming for stratified LOB\n",
    "# lob_dict_in = {1:['df']}\n",
    "lob_dict_in = {1:['df'], 2:['df_90_730'], 3:['df_730_1095'], 4:['df_1095']}\n",
    "lob_df = pd.DataFrame(lob_dict_in)\n",
    "lob_df = lob_df.T.reset_index()\n",
    "lob_df.rename(columns={0: 'ds_name', 'index': 'df_num'}, inplace = True)\n",
    "\n",
    "\n",
    "# Set up affectionate Dict naming for DV\n",
    "dv_dict_in = {1:['wt_kg'], 2:['age_d'], 3:['pps_n']}\n",
    "dv_df = pd.DataFrame(dv_dict_in)\n",
    "dv_df = dv_df.T.reset_index()\n",
    "dv_df.rename(columns={0: 'dv_name', 'index':'dv_num'}, inplace=True)\n",
    "\n",
    "\n",
    "# Set up Dict for Model ID to b merge from above dict\n",
    "# lob_ds = {1:df}\n",
    "lob_ds = {1:df, 2:df_90_730, 3:df_730_1095, 4:df_1095}\n",
    "\n",
    "\n",
    "res = stat()\n",
    "data_dict = {}\n",
    "mt_data = pd.DataFrame()\n",
    "\n",
    "# Set normality statistics\n",
    "normal_on = 0\n",
    "if normal_on != 0:\n",
    "    f = open('my_normality.html','w')\n",
    "\n",
    "\n",
    "# Set up dict for Full DVs Full Rank but Catgetoricals with No PPSN\n",
    "dvs = {1:'wt_kg', 2:'age_d', 3:'pps_n'}\n",
    "# Created new list with pps removed as IV for use as a DV\n",
    "# for i in cat:\n",
    "for i in cat_nopps:\n",
    "    for k1,v1 in dvs.items():\n",
    "        for k, v in lob_ds.items():\n",
    "            anova_df = df.loc[:,[v1,i]].dropna()\n",
    "            # res.tukey_hsd(df=v, res_var=v1, xfac_var=i, anova_model=v1 + ' ~ C(' + i + ')')\n",
    "            res.tukey_hsd(df=anova_df, res_var=v1, xfac_var=i, anova_model=v1 + ' ~ C(' + i + ')')\n",
    "            #Normality Section\n",
    "            if normal_on !=0:\n",
    "                my_shapiro = shapiro(res.anova_std_residuals)\n",
    "                my_shapiro = str(my_shapiro)\n",
    "                my_ks = kstest(res.anova_std_residuals, 'norm')\n",
    "                my_ks = str(my_ks)\n",
    "\n",
    "                # residuals qq\n",
    "                sm.qqplot(res.anova_std_residuals,line='45')\n",
    "                plt.xlabel('Theoretic Quantiles '+ v1+ ' ' + i)\n",
    "                plt.ylabel(\"Standardized Residuals\")\n",
    "                my_fig0 = plt\n",
    "                encoded0 = fig_to_base64(my_fig0)\n",
    "                my_html0 = '<img src=\"data:image/png;base64, {}\">'.format(encoded0.decode('utf-8'))\n",
    "                f.write(\"The residuals normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html0)\n",
    "\n",
    "                # residuals histogram\n",
    "                my_fig1 = plt.figure()\n",
    "                plt.hist(res.anova_model_out.resid, bins='auto', histtype='bar', ec='k') \n",
    "                plt.xlabel('Residuals' + ' ' + i + ' ' + v1)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "                # Histogram Writing\n",
    "                tmpfile = BytesIO()\n",
    "                my_fig1.savefig(tmpfile, format='png')            \n",
    "                encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "                # encoded = base64.b64encode(tmpfile.getvalue()).decode('utf8') <img src='data:image/png;base64,{{}}'>\n",
    "                my_html1 = '---' + '<img src=\\'data:image/png;base64,{}\\'>'.format(encoded) + '---'\n",
    "                f.write(\"The histogram normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html1)\n",
    "\n",
    "\n",
    "\n",
    "            tmp = res.tukey_summary\n",
    "            tmp['df_num'] = k\n",
    "            tmp['dv_num'] = k1\n",
    "            tmp['iv'] = i\n",
    "            data_dict[k] = tmp\n",
    "            mt_data = mt_data.append(data_dict[k])\n",
    "\n",
    "\n",
    "\n",
    "# Get PPN as Sole IV with Wt and Age as DV\n",
    "# Had to remove pps_n as a DV since it was an IV with dropna analysis\n",
    "dvs = {1:'wt_kg', 2:'age_d'}\n",
    "# for i in cat:\n",
    "cat_pps_n = ['pps_n']\n",
    "for i in cat_pps_n:\n",
    "    for k1,v1 in dvs.items():\n",
    "        for k, v in lob_ds.items():\n",
    "            anova_df = df.loc[:,[v1,i]].dropna()\n",
    "            res.tukey_hsd(df=anova_df, res_var=v1, xfac_var=i, anova_model=v1 + ' ~ C(' + i + ')')\n",
    "\n",
    "            #Normality Section\n",
    "            if normal_on !=0:\n",
    "                my_shapiro = shapiro(res.anova_std_residuals)\n",
    "                my_shapiro = str(my_shapiro)\n",
    "                my_ks = kstest(res.anova_std_residuals, 'norm')\n",
    "                my_ks = str(my_ks)\n",
    "                # residuals qq\n",
    "                sm.qqplot(res.anova_std_residuals,line='45')\n",
    "                plt.xlabel('Theoretic Quantiles '+ v1+ ' ' + i)\n",
    "                plt.ylabel(\"Standardized Residuals\")\n",
    "                plt.show()\n",
    "\n",
    "                my_fig3 = plt\n",
    "                encoded3 = fig_to_base64(my_fig3)\n",
    "                my_html3 = '<img src=\"data:image/png;base64, {}\">'.format(encoded3.decode('utf-8'))\n",
    "                f.write(\"The residuals normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html3)\n",
    "\n",
    "                # residuals histogram\n",
    "                my_fig4 = plt.figure()\n",
    "                plt.hist(res.anova_model_out.resid, bins='auto', histtype='bar', ec='k') \n",
    "                plt.xlabel('Residuals' + ' ' + i + ' ' + v1)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "                # Histogram Writing\n",
    "                tmpfile = BytesIO()\n",
    "                my_fig4.savefig(tmpfile, format='png')            \n",
    "                encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "                my_html4 = '---' + '<img src=\\'data:image/png;base64,{}\\'>'.format(encoded) + '---'\n",
    "                f.write(\"The histogram normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html4)\n",
    "\n",
    "\n",
    "            tmp = res.tukey_summary\n",
    "            tmp['df_num'] = k\n",
    "            tmp['dv_num'] = k1\n",
    "            tmp['iv'] = i\n",
    "            data_dict[k] = tmp\n",
    "            mt_data = mt_data.append(data_dict[k])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get Age LT 730 as Sole IV with Wt and wt and PPS\n",
    "# Had to remove pps_n as a DV since it was an IV with dropna analysis\n",
    "dvs = {1:'wt_kg', 2:'pps_n'}\n",
    "# for i in cat:\n",
    "cat_age_lt_730_n = ['age_lt_730']\n",
    "for i in cat_age_lt_730_n:\n",
    "    for k1,v1 in dvs.items():\n",
    "        for k, v in lob_ds.items():\n",
    "            anova_df = df.loc[:,[v1,i]].dropna()\n",
    "            res.tukey_hsd(df=anova_df, res_var=v1, xfac_var=i, anova_model=v1 + ' ~ C(' + i + ')')\n",
    "\n",
    "            #Normality Section\n",
    "            if normal_on !=0:\n",
    "                my_shapiro = shapiro(res.anova_std_residuals)\n",
    "                my_shapiro = str(my_shapiro)\n",
    "                my_ks = kstest(res.anova_std_residuals, 'norm')\n",
    "                my_ks = str(my_ks)\n",
    "                # residuals qq\n",
    "                sm.qqplot(res.anova_std_residuals,line='45')\n",
    "                plt.xlabel('Theoretic Quantiles '+ v1+ ' ' + i)\n",
    "                plt.ylabel(\"Standardized Residuals\")\n",
    "                plt.show()\n",
    "\n",
    "                my_fig5 = plt\n",
    "                encoded5 = fig_to_base64(my_fig5)\n",
    "                my_html5 = '<img src=\"data:image/png;base64, {}\">'.format(encoded5.decode('utf-8'))\n",
    "                f.write(\"The residuals normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html5)\n",
    "\n",
    "                # residuals histogram\n",
    "                my_fig6 = plt.figure()\n",
    "                plt.hist(res.anova_model_out.resid, bins='auto', histtype='bar', ec='k') \n",
    "                plt.xlabel('Residuals' + ' ' + i + ' ' + v1)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.show()\n",
    "\n",
    "                # Histogram Writing\n",
    "                tmpfile = BytesIO()\n",
    "                my_fig6.savefig(tmpfile, format='png')            \n",
    "                encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "                my_html6 = '---' + '<img src=\\'data:image/png;base64,{}\\'>'.format(encoded) + '---'\n",
    "                f.write(\"The histogram normality plot is based upon {} and {} and KS is {} and Shapiro is {}.\".format(v1, i, my_ks, my_shapiro))\n",
    "                f.write(my_html6)\n",
    "\n",
    "\n",
    "            tmp = res.tukey_summary\n",
    "            tmp['df_num'] = k\n",
    "            tmp['dv_num'] = k1\n",
    "            tmp['iv'] = i\n",
    "            data_dict[k] = tmp\n",
    "            mt_data = mt_data.append(data_dict[k])\n",
    "\n",
    "\n",
    "\n",
    "diffs = pd.merge(mt_data, lob_df, on=[\"df_num\"])\n",
    "diffs = pd.merge(diffs, dv_df, on=[\"dv_num\"])\n",
    "diffs\n",
    "\n",
    "if normal_on != 0:\n",
    "    f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA of continuous vars with top 5 Breed IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Lower</th>\n",
       "      <th>Upper</th>\n",
       "      <th>q-value</th>\n",
       "      <th>p-value</th>\n",
       "      <th>df_num</th>\n",
       "      <th>dv_num</th>\n",
       "      <th>iv</th>\n",
       "      <th>ds_name</th>\n",
       "      <th>dv_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.312477</td>\n",
       "      <td>1.257804</td>\n",
       "      <td>3.367149</td>\n",
       "      <td>8.526555</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>-1.254606</td>\n",
       "      <td>1.554849</td>\n",
       "      <td>0.415590</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.537859</td>\n",
       "      <td>1.525402</td>\n",
       "      <td>3.550316</td>\n",
       "      <td>9.747758</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.102148</td>\n",
       "      <td>-1.066836</td>\n",
       "      <td>1.271132</td>\n",
       "      <td>0.339809</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.462598</td>\n",
       "      <td>1.158432</td>\n",
       "      <td>3.766765</td>\n",
       "      <td>7.343016</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.225382</td>\n",
       "      <td>-0.642178</td>\n",
       "      <td>1.092943</td>\n",
       "      <td>1.010261</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.414625</td>\n",
       "      <td>1.368625</td>\n",
       "      <td>3.460624</td>\n",
       "      <td>8.977021</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.687981</td>\n",
       "      <td>1.417711</td>\n",
       "      <td>3.958250</td>\n",
       "      <td>8.228943</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.047974</td>\n",
       "      <td>-1.350253</td>\n",
       "      <td>1.446201</td>\n",
       "      <td>0.133426</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.640007</td>\n",
       "      <td>1.636588</td>\n",
       "      <td>3.643426</td>\n",
       "      <td>10.231440</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>wt_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>53.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>681.972973</td>\n",
       "      <td>-2208.735601</td>\n",
       "      <td>3572.681547</td>\n",
       "      <td>0.917438</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>913.287027</td>\n",
       "      <td>-2936.871577</td>\n",
       "      <td>4763.445631</td>\n",
       "      <td>0.922449</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>314.843103</td>\n",
       "      <td>-2460.158596</td>\n",
       "      <td>3089.844802</td>\n",
       "      <td>0.441210</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1236.579659</td>\n",
       "      <td>-1967.438905</td>\n",
       "      <td>4440.598222</td>\n",
       "      <td>1.500863</td>\n",
       "      <td>0.802942</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1595.260000</td>\n",
       "      <td>-1979.275762</td>\n",
       "      <td>5169.795762</td>\n",
       "      <td>1.735505</td>\n",
       "      <td>0.709222</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>367.129870</td>\n",
       "      <td>-2010.730317</td>\n",
       "      <td>2744.990057</td>\n",
       "      <td>0.600409</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1918.552632</td>\n",
       "      <td>-948.382944</td>\n",
       "      <td>4785.488207</td>\n",
       "      <td>2.602374</td>\n",
       "      <td>0.353604</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>62.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1228.130130</td>\n",
       "      <td>-2253.499414</td>\n",
       "      <td>4709.759674</td>\n",
       "      <td>1.371752</td>\n",
       "      <td>0.854514</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>62.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>323.292632</td>\n",
       "      <td>-3509.049317</td>\n",
       "      <td>4155.634580</td>\n",
       "      <td>0.328054</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>61.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1551.422761</td>\n",
       "      <td>-1198.805950</td>\n",
       "      <td>4301.651472</td>\n",
       "      <td>2.193690</td>\n",
       "      <td>0.526212</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>age_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>53.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.597696</td>\n",
       "      <td>-0.312200</td>\n",
       "      <td>1.507592</td>\n",
       "      <td>2.554481</td>\n",
       "      <td>0.373366</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.760811</td>\n",
       "      <td>-0.451087</td>\n",
       "      <td>1.972709</td>\n",
       "      <td>2.441319</td>\n",
       "      <td>0.421565</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.148473</td>\n",
       "      <td>0.274998</td>\n",
       "      <td>2.021948</td>\n",
       "      <td>5.113098</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>53.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.573969</td>\n",
       "      <td>-0.434546</td>\n",
       "      <td>1.582484</td>\n",
       "      <td>2.213195</td>\n",
       "      <td>0.518420</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>35.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.163115</td>\n",
       "      <td>-0.962026</td>\n",
       "      <td>1.288256</td>\n",
       "      <td>0.563768</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.550777</td>\n",
       "      <td>-0.197692</td>\n",
       "      <td>1.299246</td>\n",
       "      <td>2.861647</td>\n",
       "      <td>0.258075</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.023727</td>\n",
       "      <td>-0.878686</td>\n",
       "      <td>0.926140</td>\n",
       "      <td>0.102249</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>62.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.387662</td>\n",
       "      <td>-0.708235</td>\n",
       "      <td>1.483560</td>\n",
       "      <td>1.375617</td>\n",
       "      <td>0.852968</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>62.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.186842</td>\n",
       "      <td>-1.019448</td>\n",
       "      <td>1.393132</td>\n",
       "      <td>0.602334</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>61.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.574504</td>\n",
       "      <td>-0.291173</td>\n",
       "      <td>1.440182</td>\n",
       "      <td>2.580781</td>\n",
       "      <td>0.362453</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>breed_n</td>\n",
       "      <td>df</td>\n",
       "      <td>pps_n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group1  group2         Diff        Lower        Upper    q-value  \\\n",
       "0     53.0    35.0     2.312477     1.257804     3.367149   8.526555   \n",
       "1     53.0    62.0     0.150122    -1.254606     1.554849   0.415590   \n",
       "2     53.0    61.0     2.537859     1.525402     3.550316   9.747758   \n",
       "3     53.0    38.0     0.102148    -1.066836     1.271132   0.339809   \n",
       "4     35.0    62.0     2.462598     1.158432     3.766765   7.343016   \n",
       "5     35.0    61.0     0.225382    -0.642178     1.092943   1.010261   \n",
       "6     35.0    38.0     2.414625     1.368625     3.460624   8.977021   \n",
       "7     62.0    61.0     2.687981     1.417711     3.958250   8.228943   \n",
       "8     62.0    38.0     0.047974    -1.350253     1.446201   0.133426   \n",
       "9     61.0    38.0     2.640007     1.636588     3.643426  10.231440   \n",
       "10    53.0    35.0   681.972973 -2208.735601  3572.681547   0.917438   \n",
       "11    53.0    62.0   913.287027 -2936.871577  4763.445631   0.922449   \n",
       "12    53.0    61.0   314.843103 -2460.158596  3089.844802   0.441210   \n",
       "13    53.0    38.0  1236.579659 -1967.438905  4440.598222   1.500863   \n",
       "14    35.0    62.0  1595.260000 -1979.275762  5169.795762   1.735505   \n",
       "15    35.0    61.0   367.129870 -2010.730317  2744.990057   0.600409   \n",
       "16    35.0    38.0  1918.552632  -948.382944  4785.488207   2.602374   \n",
       "17    62.0    61.0  1228.130130 -2253.499414  4709.759674   1.371752   \n",
       "18    62.0    38.0   323.292632 -3509.049317  4155.634580   0.328054   \n",
       "19    61.0    38.0  1551.422761 -1198.805950  4301.651472   2.193690   \n",
       "20    53.0    35.0     0.597696    -0.312200     1.507592   2.554481   \n",
       "21    53.0    62.0     0.760811    -0.451087     1.972709   2.441319   \n",
       "22    53.0    61.0     1.148473     0.274998     2.021948   5.113098   \n",
       "23    53.0    38.0     0.573969    -0.434546     1.582484   2.213195   \n",
       "24    35.0    62.0     0.163115    -0.962026     1.288256   0.563768   \n",
       "25    35.0    61.0     0.550777    -0.197692     1.299246   2.861647   \n",
       "26    35.0    38.0     0.023727    -0.878686     0.926140   0.102249   \n",
       "27    62.0    61.0     0.387662    -0.708235     1.483560   1.375617   \n",
       "28    62.0    38.0     0.186842    -1.019448     1.393132   0.602334   \n",
       "29    61.0    38.0     0.574504    -0.291173     1.440182   2.580781   \n",
       "\n",
       "     p-value  df_num  dv_num       iv ds_name dv_name  \n",
       "0   0.001000       1       1  breed_n      df   wt_kg  \n",
       "1   0.900000       1       1  breed_n      df   wt_kg  \n",
       "2   0.001000       1       1  breed_n      df   wt_kg  \n",
       "3   0.900000       1       1  breed_n      df   wt_kg  \n",
       "4   0.001000       1       1  breed_n      df   wt_kg  \n",
       "5   0.900000       1       1  breed_n      df   wt_kg  \n",
       "6   0.001000       1       1  breed_n      df   wt_kg  \n",
       "7   0.001000       1       1  breed_n      df   wt_kg  \n",
       "8   0.900000       1       1  breed_n      df   wt_kg  \n",
       "9   0.001000       1       1  breed_n      df   wt_kg  \n",
       "10  0.900000       1       2  breed_n      df   age_d  \n",
       "11  0.900000       1       2  breed_n      df   age_d  \n",
       "12  0.900000       1       2  breed_n      df   age_d  \n",
       "13  0.802942       1       2  breed_n      df   age_d  \n",
       "14  0.709222       1       2  breed_n      df   age_d  \n",
       "15  0.900000       1       2  breed_n      df   age_d  \n",
       "16  0.353604       1       2  breed_n      df   age_d  \n",
       "17  0.854514       1       2  breed_n      df   age_d  \n",
       "18  0.900000       1       2  breed_n      df   age_d  \n",
       "19  0.526212       1       2  breed_n      df   age_d  \n",
       "20  0.373366       1       3  breed_n      df   pps_n  \n",
       "21  0.421565       1       3  breed_n      df   pps_n  \n",
       "22  0.003367       1       3  breed_n      df   pps_n  \n",
       "23  0.518420       1       3  breed_n      df   pps_n  \n",
       "24  0.900000       1       3  breed_n      df   pps_n  \n",
       "25  0.258075       1       3  breed_n      df   pps_n  \n",
       "26  0.900000       1       3  breed_n      df   pps_n  \n",
       "27  0.852968       1       3  breed_n      df   pps_n  \n",
       "28  0.900000       1       3  breed_n      df   pps_n  \n",
       "29  0.362453       1       3  breed_n      df   pps_n  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res = stat()\n",
    "data_dict = {}\n",
    "t5_breed_data = pd.DataFrame()\n",
    "\n",
    "dvs = {1:'wt_kg', 2:'age_d', 3:'pps_n'}\n",
    "\n",
    "cat_breed = ['breed_n']\n",
    "ds_breed = {1:breed_t5}\n",
    "\n",
    "for i in cat_breed:\n",
    "    for k1,v1 in dvs.items():\n",
    "        for k, v in ds_breed.items():\n",
    "            res.tukey_hsd(df=v, res_var=v1, xfac_var=i, anova_model=v1 + ' ~ C(' + i + ')')\n",
    "            tmp = res.tukey_summary\n",
    "            tmp['df_num'] = k\n",
    "            tmp['dv_num'] = k1\n",
    "            tmp['iv'] = i\n",
    "            data_dict[k] = tmp\n",
    "            t5_breed_data = t5_breed_data.append(data_dict[k])\n",
    "\n",
    "\n",
    "breed_diffs = pd.merge(t5_breed_data, lob_df, on=[\"df_num\"])\n",
    "breed_diffs = pd.merge(breed_diffs, dv_df, on=[\"dv_num\"])\n",
    "breed_diffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_sheets = ['cat_one' 'cross_cat' 'm_desc' 'm_class' 'diffs' 'breed_diffs']\n",
    "\n",
    "writer = pd.ExcelWriter('ephss_out_20230107.xlsx', engine = 'xlsxwriter')\n",
    "# for i in my_sheets:\n",
    "#     [i].to_excel(writer,sheet_name=i)\n",
    "\n",
    "# Data In\n",
    "df.to_excel(writer,sheet_name='df')\n",
    "df_90_730.to_excel(writer,sheet_name='df_90_730')\n",
    "df_730_1095.to_excel(writer,sheet_name='df_730_1095')\n",
    "df_1095.to_excel(writer,sheet_name='df_1095')\n",
    "# One Way Freq\n",
    "cat_df.to_excel(writer,sheet_name='cat_df')\n",
    "cat_df_90_730.to_excel(writer,sheet_name='cat_df_90_730')\n",
    "cat_df_730_1095.to_excel(writer,sheet_name='cat_df_730_1095')\n",
    "cat_df_1095.to_excel(writer,sheet_name='cat_df_1095')\n",
    "# Cross Tab Frequency\n",
    "cross_cat.to_excel(writer,sheet_name='cross_cat')\n",
    "cross_cat_df_90_730.to_excel(writer,sheet_name='cross_cat_df_90_730')\n",
    "cross_cat_df_730_1095.to_excel(writer,sheet_name='cross_cat_df_730_1095')\n",
    "cross_cat_df_1095.to_excel(writer,sheet_name='cross_cat_df_1095')\n",
    "# Means overall\n",
    "m_desc_df.to_excel(writer,sheet_name='m_desc_df')\n",
    "m_desc_df_90_730.to_excel(writer,sheet_name='m_desc_df_90_730')\n",
    "m_desc_df_730_1095.to_excel(writer,sheet_name='m_desc_df_730_1095')\n",
    "m_desc_df_1095.to_excel(writer,sheet_name='m_desc_df_1095')\n",
    "# Means with class\n",
    "m_class_df.to_excel(writer,sheet_name='m_class_df')\n",
    "m_class_df_90_730.to_excel(writer,sheet_name='m_class_df_90_730')\n",
    "m_class_df_730_1095.to_excel(writer,sheet_name='m_class_df_730_1095')\n",
    "m_class_df_1095.to_excel(writer,sheet_name='m_class_df_1095')\n",
    "# Diffs\n",
    "diffs.to_excel(writer,sheet_name='diffs')\n",
    "breed_diffs.to_excel(writer,sheet_name='breed_diffs')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "# Single df write\n",
    "# my_sheets = ['df']\n",
    "# writer = pd.ExcelWriter('df.xlsx', engine = 'xlsxwriter')\n",
    "# df.to_excel(writer,sheet_name='data')\n",
    "# writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comparing: Pct 1: 0.271, Pct 2: 0.258 Num 1: 220, Denom 1: 812, Num 2: 16, Denom 2: 62, Var 1: country, Denom Levels: 0/1, Num Levels: 1/1, P-Value is: 0.826']\n",
      "['Comparing: Pct 1: 0.271, Pct 2: 0.292 Num 1: 220, Denom 1: 812, Num 2: 14, Denom 2: 48, Var 1: country, Denom Levels: 0/2, Num Levels: 1/1, P-Value is: 0.754']\n",
      "['Comparing: Pct 1: 0.271, Pct 2: 0.122 Num 1: 220, Denom 1: 812, Num 2: 19, Denom 2: 156, Var 1: country, Denom Levels: 0/3, Num Levels: 1/1, P-Value is: 0.000']\n",
      "['Comparing: Pct 1: 0.087, Pct 2: 0.097 Num 1: 71, Denom 1: 812, Num 2: 6, Denom 2: 62, Var 1: country, Denom Levels: 0/1, Num Levels: 2/2, P-Value is: 0.803']\n",
      "['Comparing: Pct 1: 0.087, Pct 2: 0.146 Num 1: 71, Denom 1: 812, Num 2: 7, Denom 2: 48, Var 1: country, Denom Levels: 0/2, Num Levels: 2/2, P-Value is: 0.171']\n",
      "['Comparing: Pct 1: 0.087, Pct 2: 0.051 Num 1: 71, Denom 1: 812, Num 2: 8, Denom 2: 156, Var 1: country, Denom Levels: 0/3, Num Levels: 2/2, P-Value is: 0.131']\n",
      "['Comparing: Pct 1: 0.123, Pct 2: 0.097 Num 1: 100, Denom 1: 812, Num 2: 6, Denom 2: 62, Var 1: country, Denom Levels: 0/1, Num Levels: 3/2, P-Value is: 0.540']\n",
      "['Comparing: Pct 1: 0.123, Pct 2: 0.146 Num 1: 100, Denom 1: 812, Num 2: 7, Denom 2: 48, Var 1: country, Denom Levels: 0/2, Num Levels: 3/2, P-Value is: 0.644']\n",
      "['Comparing: Pct 1: 0.123, Pct 2: 0.051 Num 1: 100, Denom 1: 812, Num 2: 8, Denom 2: 156, Var 1: country, Denom Levels: 0/3, Num Levels: 3/2, P-Value is: 0.009']\n",
      "['Comparing: Pct 1: 0.258, Pct 2: 0.292 Num 1: 16, Denom 1: 62, Num 2: 14, Denom 2: 48, Var 1: country, Denom Levels: 1/2, Num Levels: 1/1, P-Value is: 0.695']\n",
      "['Comparing: Pct 1: 0.258, Pct 2: 0.122 Num 1: 16, Denom 1: 62, Num 2: 19, Denom 2: 156, Var 1: country, Denom Levels: 1/3, Num Levels: 1/1, P-Value is: 0.013']\n",
      "['Comparing: Pct 1: 0.097, Pct 2: 0.146 Num 1: 6, Denom 1: 62, Num 2: 7, Denom 2: 48, Var 1: country, Denom Levels: 1/2, Num Levels: 2/2, P-Value is: 0.429']\n",
      "['Comparing: Pct 1: 0.097, Pct 2: 0.051 Num 1: 6, Denom 1: 62, Num 2: 8, Denom 2: 156, Var 1: country, Denom Levels: 1/3, Num Levels: 2/2, P-Value is: 0.216']\n",
      "['Comparing: Pct 1: 0.258, Pct 2: 0.104 Num 1: 16, Denom 1: 62, Num 2: 5, Denom 2: 48, Var 1: country, Denom Levels: 1/2, Num Levels: 3/3, P-Value is: 0.042']\n",
      "['Comparing: Pct 1: 0.258, Pct 2: 0.103 Num 1: 16, Denom 1: 62, Num 2: 16, Denom 2: 156, Var 1: country, Denom Levels: 1/3, Num Levels: 3/3, P-Value is: 0.003']\n",
      "['Comparing: Pct 1: 0.292, Pct 2: 0.122 Num 1: 14, Denom 1: 48, Num 2: 19, Denom 2: 156, Var 1: country, Denom Levels: 2/3, Num Levels: 1/1, P-Value is: 0.005']\n",
      "['Comparing: Pct 1: 0.146, Pct 2: 0.051 Num 1: 7, Denom 1: 48, Num 2: 8, Denom 2: 156, Var 1: country, Denom Levels: 2/3, Num Levels: 2/2, P-Value is: 0.028']\n",
      "['Comparing: Pct 1: 0.104, Pct 2: 0.103 Num 1: 5, Denom 1: 48, Num 2: 16, Denom 2: 156, Var 1: country, Denom Levels: 2/3, Num Levels: 3/3, P-Value is: 0.975']\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import numpy as np\n",
    "\n",
    "# can we assume anything from our sample\n",
    "significance = 0.025\n",
    "\n",
    "def my_z_test(denom_in, num_in, denom_grp_1, denom_grp_2, grp_1, grp_2):\n",
    "    proportion_data = pd.DataFrame()\n",
    "    # Step 1 country is in index\n",
    "    tmp = pd.crosstab(df[denom_in].fillna('.'),df[num_in].fillna('.'))\n",
    "    tmp = tmp.drop(columns=['.'])\n",
    "    tmp.reset_index(inplace=True)\n",
    "    # Work on values\n",
    "\n",
    "    tmp_1a = tmp.loc[tmp[denom_in] == denom_grp_1] # Country is 0\n",
    "    tmp_1a = tmp_1a.drop([denom_in], axis=1)\n",
    "    denom_1 = tmp_1a.loc[denom_grp_1].sum() # Country is 0\n",
    "    # Get numerator from single row\n",
    "    tmp_1b = tmp_1a[[grp_1]].reset_index() # pps_n is 1\n",
    "    tmp_1b = tmp_1b.drop(['index'], axis=1)\n",
    "    # Get only column and sum, if no sum will take full dataframe and not just numerator\n",
    "    num_1 = tmp_1b.loc[0].sum()\n",
    "    tup1 = (num_1, denom_1)\n",
    "    # print(\"tup1\")\n",
    "    # print(tup1)\n",
    "    pct_1 = num_1 / denom_1\n",
    "\n",
    "\n",
    "    # Get single row to be analyzed for upcoming comparision\n",
    "    tmp_2a = tmp.loc[tmp[denom_in] == denom_grp_2]\n",
    "    tmp_2a = tmp_2a.drop([denom_in], axis=1)\n",
    "    denom_2 = tmp_2a.loc[denom_grp_2].sum() # Country 1\n",
    "    # Get numerator from single row e.g. pps_n 1 is first column, 2 is second column\n",
    "    tmp_2b = tmp_2a[[grp_2]].reset_index() # pps_n is 1\n",
    "    tmp_2b = tmp_2b.drop(['index'], axis=1)\n",
    "    num_2 = tmp_2b.loc[0].sum() \n",
    "    tup2 = (num_2, denom_2)\n",
    "    # print(\"tup2\")\n",
    "    # print(tup2)\n",
    "    pct_2 = num_2 / denom_2\n",
    "    sample_success_a, sample_size_a = tup1\n",
    "    sample_success_b, sample_size_b = tup2\n",
    "    successes = np.array([sample_success_a, sample_success_b])\n",
    "    samples = np.array([sample_size_a, sample_size_b])\n",
    "\n",
    "    # note, no need for a Ho value here - it's derived from the other parameters\n",
    "    stat, p_value = proportions_ztest(count=successes, nobs=samples, alternative='two-sided')\n",
    "\n",
    "\n",
    "    # narrative = ('Comparing: Pct 1: %0.3f, Pct 2: %0.3f' % (pct_1, pct_2) + \" Var 1: \" + denom_in + \" Var 2: \" + num_in)\n",
    "    narrative = [('Comparing: Pct 1: %0.3f, Pct 2: %0.3f Num 1: %0.0f, Denom 1: %0.0f, Num 2: %0.0f, Denom 2: %0.0f'% (pct_1, pct_2, num_1, denom_1, num_2, denom_2) + \n",
    "    \", Var 1: \" + denom_in + ', Denom Levels: %0.0f/%0.0f' % (denom_grp_1, denom_grp_2) + ', Num Levels: %0.0f/%0.0f' % (grp_1, grp_2) + ', P-Value is: %0.3f' % (p_value))]\n",
    "    #  \", Var 1: \" + denom_in + \" \" + denom_grp_1 + \"/\" + denom_grp_2 + \", Var 2: \" + num_in + grp_1 + \"/\" + grp_2 + ', P-Value is: %0.3f' % (p_value))]\n",
    "    #  \"/\" + denom_grp_2 + \", Var 2: \" + num_in + grp_1 + \"/\" + grp_2 + \n",
    "    print(narrative)\n",
    "    tmp_df = pd.DataFrame({'narrative' : narrative, 'p_value' : p_value})\n",
    "\n",
    "    proportion_data = pd.concat([my_df,tmp_df])\n",
    "    # proportion_data = proportion_data.append(tmp_df)\n",
    "\n",
    "    # return narrative, p_value\n",
    "    return proportion_data\n",
    "\n",
    "\n",
    "my_df = pd.DataFrame()\n",
    "\n",
    "# The first two are comparing pairwise for the first variable and should be different e.g. country 0/1, 0,2, 0/3\n",
    "# The last two should be the same since we are comparing the same level of the second variable within the pairwise first variable e.g. country 0 pps 1 vs. country 1 pps 1\n",
    "\n",
    "# Country 0 vs 1, 2, 3 | PPS 1\n",
    "my_df = my_z_test('country', 'pps_n', 0, 1, 1, 1) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 1), col val 1 (pps 1), col val 2 (pps 1)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 2, 1, 1) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 2), col val 1 (pps 1), col val 2 (pps 1)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 3, 1, 1) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 3), col val 1 (pps 1), col val 2 (pps 1)\n",
    "# Country 0 vs 1, 2, 3 | PPS 2\n",
    "my_df = my_z_test('country', 'pps_n', 0, 1, 2, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 1), col val 1 (pps 2), col val 2 (pps 2)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 2, 2, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 2), col val 1 (pps 2), col val 2 (pps 2)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 3, 2, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 3), col val 1 (pps 2), col val 2 (pps 2)\n",
    "# Country 0 vs 1, 2, 3 | PPS 3\n",
    "my_df = my_z_test('country', 'pps_n', 0, 1, 3, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 1), col val 1 (pps 3), col val 2 (pps 3)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 2, 3, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 2), col val 1 (pps 3), col val 2 (pps 3)\n",
    "my_df = my_z_test('country', 'pps_n', 0, 3, 3, 2) # row var (country), col var (pps_n), row val 1 (country 0), row val 2 (country 3), col val 1 (pps 3), col val 2 (pps 3)\n",
    "# Country 1 vs 2, 3 | PPS 1 and 2 and 3\n",
    "my_df = my_z_test('country', 'pps_n', 1, 2, 1, 1)\n",
    "my_df = my_z_test('country', 'pps_n', 1, 3, 1, 1)\n",
    "my_df = my_z_test('country', 'pps_n', 1, 2, 2, 2)\n",
    "my_df = my_z_test('country', 'pps_n', 1, 3, 2, 2)\n",
    "my_df = my_z_test('country', 'pps_n', 1, 2, 3, 3)\n",
    "my_df = my_z_test('country', 'pps_n', 1, 3, 3, 3)\n",
    "# Country 0 vs 1, 2, 3 | PPS 1 and 2 and 3\n",
    "my_df = my_z_test('country', 'pps_n', 2, 3, 1, 1)\n",
    "my_df = my_z_test('country', 'pps_n', 2, 3, 2, 2)\n",
    "my_df = my_z_test('country', 'pps_n', 2, 3, 3, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob TTDIST from Excel Proportionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5073350414125459"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Excel Example\n",
    "# https://stackoverflow.com/questions/56991534/excels-tdist-in-scipy\n",
    "# from scipy import stats\n",
    "stats.t.sf(.66299299354, df=9999999999999) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_stat: -0.220, p_value: 0.826\n",
      "Fail to reject the null hypothesis - we have nothing else to say\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/sonalake/blog-hypothesis-testing/blob/master/examples/proportions/2-sample-z-test.py\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import numpy as np\n",
    "\n",
    "# can we assume anything from our sample\n",
    "significance = 0.025\n",
    "\n",
    "# our samples;  note - the samples do not need to be the same size\n",
    "\n",
    "# my_a = (16, 62)\n",
    "# my_b = (220, 812)\n",
    "# sample_success_a, sample_size_a = my_a\n",
    "# sample_success_b, sample_size_b = my_b\n",
    "\n",
    "# here 410 samples out of 500 were \"ok\"\n",
    "sample_success_a, sample_size_a = (16, 62)\n",
    "# here 379 samples out of 400 were \"ok\"\n",
    "sample_success_b, sample_size_b = (220, 812)\n",
    "\n",
    "# check our sample against Ho for Ha != Ho\n",
    "successes = np.array([sample_success_a, sample_success_b])\n",
    "samples = np.array([sample_size_a, sample_size_b])\n",
    "\n",
    "# note, no need for a Ho value here - it's derived from the other parameters\n",
    "stat, p_value = proportions_ztest(count=successes, nobs=samples, alternative='two-sided')\n",
    "\n",
    "# report\n",
    "print('z_stat: %0.3f, p_value: %0.3f' % (stat, p_value))\n",
    "\n",
    "if p_value > significance:\n",
    "    print(\"Fail to reject the null hypothesis - we have nothing else to say\")\n",
    "else:\n",
    "    print(\"Reject the null hypothesis - suggest the alternative hypothesis is true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2frame(estr, sep = ',', lineterm = '\\n', set_header = True):\n",
    "    dat = [x.split(sep) for x in estr.split(lineterm)][1:-1]\n",
    "    df = pd.DataFrame(dat)\n",
    "    if set_header:\n",
    "        df = df.T.set_index(0, drop = True).T # flip, set ix, flip back\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tup1\n",
      "(220, 812)\n",
      "denom_2\n",
      "62\n",
      "tup2\n",
      "(16, 62)\n",
      "z_stat: 0.220, p_value: 0.826\n",
      "Fail to reject the null hypothesis - we have nothing else to say\n"
     ]
    }
   ],
   "source": [
    "# Step 1 country is in index\n",
    "tmp = pd.crosstab(df['country'].fillna('.'),df['pps_n'].fillna('.'))\n",
    "tmp = tmp.drop(columns=['.'])\n",
    "tmp.reset_index(inplace=True)\n",
    "# Get country out of index\n",
    "# print(\"tmp_a2\")\n",
    "# print(tmp_a.head(10))\n",
    "\n",
    "\n",
    "# Get single row to be analyzed for upcoming comparision\n",
    "tmp_1a = tmp.loc[tmp['country'] == 0]\n",
    "tmp_1a = tmp_1a.drop(['country'], axis=1)\n",
    "# print(\"tmp_b\")\n",
    "# print(tmp_b.head(10))\n",
    "denom_1 = tmp_1a.loc[0].sum() # Country 0\n",
    "# print(\"denom_0\")\n",
    "# print(denom_0)\n",
    "# type(denom_0)\n",
    "\n",
    "# Get numerator from single row\n",
    "tmp_1b = tmp_1a[[1]].reset_index() # pps_n is 1\n",
    "tmp_1b = tmp_1b.drop(['index'], axis=1)\n",
    "# print(\"tmp_c\")\n",
    "# print(tmp_c.head(10))\n",
    "# Get only column and sum, if no sum will take full dataframe and not just numerator\n",
    "num_1 = tmp_1b.loc[0].sum() \n",
    "# print(\"num_0\")\n",
    "# print(num_0)\n",
    "# print(type(num_0))\n",
    "tup1 = (num_1, denom_1)\n",
    "print(\"tup1\")\n",
    "print(tup1)\n",
    "type(tup1)\n",
    "\n",
    "\n",
    "# Get single row to be analyzed for upcoming comparision\n",
    "tmp_2a = tmp.loc[tmp['country'] == 1]\n",
    "tmp_2a = tmp_2a.drop(['country'], axis=1)\n",
    "# print(\"tmp_2a\")\n",
    "# print(tmp_2a.head(10))\n",
    "denom_2 = tmp_2a.loc[1].sum() # Country 1\n",
    "print(\"denom_2\")\n",
    "print(denom_2)\n",
    "# Get numerator from single row e.g. pps_n 1 is first column, 2 is second column\n",
    "tmp_2b = tmp_2a[[1]].reset_index() # pps_n is 1\n",
    "tmp_2b = tmp_2b.drop(['index'], axis=1)\n",
    "num_2 = tmp_2b.loc[0].sum() \n",
    "tup2 = (num_2, denom_2)\n",
    "print(\"tup2\")\n",
    "print(tup2)\n",
    "type(tup2)\n",
    "\n",
    "\n",
    "\n",
    "sample_success_a, sample_size_a = tup1\n",
    "# here 379 samples out of 400 were \"ok\"\n",
    "sample_success_b, sample_size_b = tup2\n",
    "\n",
    "# check our sample against Ho for Ha != Ho\n",
    "successes = np.array([sample_success_a, sample_success_b])\n",
    "samples = np.array([sample_size_a, sample_size_b])\n",
    "\n",
    "# note, no need for a Ho value here - it's derived from the other parameters\n",
    "stat, p_value = proportions_ztest(count=successes, nobs=samples, alternative='two-sided')\n",
    "\n",
    "# report\n",
    "print('z_stat: %0.3f, p_value: %0.3f' % (stat, p_value))\n",
    "\n",
    "if p_value > significance:\n",
    "    print(\"Fail to reject the null hypothesis - we have nothing else to say\")\n",
    "else:\n",
    "    print(\"Reject the null hypothesis - suggest the alternative hypothesis is true\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "074388e44af23a57e74ba7d2a1ea7b051c27ae90374df84d0c6ba2eba547c336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
